{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdfbcacc",
   "metadata": {},
   "source": [
    "## What is artificial intelligence?\n",
    "\n",
    "Usually due to lack of knowledge, we usually call artificial intelligence to any operation that is carried out by a computer. However, this is far from the truth. Artificial intelligence is nourished by different areas, the most popular: data science, deep learning and deep learning.\n",
    "\n",
    "Let's be clear about something, data science is not artificial intelligence, the opposite is not true either, however, you can solve a problem with data science without touching artificial intelligence at any time, the opposite cannot always be carried out.\n",
    "\n",
    "Let's first understand the following diagram:\n",
    "\n",
    "<img src=\"images/components.png\"/>\n",
    "\n",
    "Natural language processing, image recognition, the process of going from text to speech, language translation, among other areas, are what together we should call artificial intelligence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e52ee5",
   "metadata": {},
   "source": [
    "## When do we have an AI problem?\n",
    "As we have indicated, it is possible to solve a computational problem simply using data science, applying for example an excellent visualization of the data. We will have an AI problem in cases where we want a machine to carry out very specific tasks in an automated way, where using data we can get that machine to carry out the work for which it was programmed in a correct way and with high precision.\n",
    "\n",
    "\n",
    "## Areas where Artificial Intelligence can be applied\n",
    "Nowadays, there are few areas where algorithms that are related to artificial intelligence are not carried out, from very repetitive tasks such as tightening thousands of screws in one piece to areas where the human is not able to overcome these algorithms such as disease identification in X-ray or microscopic images.\n",
    "\n",
    "At visionanalytics.ai, during the 2020 Madrid strong lockdown caused by the COVID-19 pandemic, we developed an algorithm capable of differentiating X-ray images of patients from images of patients infected with pneumonia caused by COVID-19, in addition From this, the AI was able to pinpoint the affected area:\n",
    "\n",
    "<img src=\"images/covid.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "Other solutions were also implemented to reduce the risk of contagion by COVID-19 in different public spaces with a mask detector:\n",
    "\n",
    "<img src=\"images/mask.png\" width=\"500\"/>\n",
    "\n",
    "The applications as you can see are innumerable, such as increasing the security of spaces by detecting guns:\n",
    "\n",
    "<img src=\"images/gun.png\" width=\"500\"/>\n",
    "\n",
    "In fact, we use a part of artificial intelligence, every time we take a “selfie” with our mobile phone and it automatically recognizes our face\n",
    "\n",
    "<img src=\"images/phone.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "O cuando usamos un filtro de instagram. El dispositivo reconoce la cara de la persona y coloca un “dibujo” en las partes adecuadas de la cara. En este caso, no solo reconoce una cara, sino además detecta la posición de cada una de las partes que la componen\n",
    "\n",
    "\n",
    "<img src=\"images/instagram.png\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40631cd6",
   "metadata": {},
   "source": [
    "## Data Science vs Machine Learning vs Deep Learning vs Artificial Intelligence\n",
    "\n",
    "It usually confuses all these terms but the reality is that although they go hand in hand, many times they do not even require one of the other.\n",
    "\n",
    "Something similar happens when we talk about Machine Learning and Artificial Intelligence, where AI is composed (among other things) of different machine learning algorithms.\n",
    "\n",
    "On the othe hand, Machine learning is the process of data analysis using an algorithm or statistical model that “learns” based on patterns within a model dataset it is exposed to. And just as a curious fact, this algorithm was introduced for the first time in 1847 by the French mathematician Augustin Louis Cauchy as a method of solving non-linear equations. And yes, the artificial intelligence we know today is based on an algorithm from 1847.\n",
    "\n",
    "Each new dataset the algorithm is exposed to helps to “train” it to achieve a certain outcome, as it adjusts its calculation and decision-making process.\n",
    "\n",
    "Therefore, we have what is seen in the image:\n",
    "\n",
    "<img src=\"images/diagram.png\" alt=\"diagram\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5db19b",
   "metadata": {},
   "source": [
    "## And finally, Big Data.\n",
    "\n",
    "Data Science is any operation that requires data management. From a simple calculation of the age mean using statistics, to complicated data transformations using relational operations (SQL). And this is where the first confusion is generated, so what is Big Data? Well, Big Data, a term very badly used in our days like artificial intelligence, is nothing more than massive data processing, that is, let's say that in data science we process data of any type and at any scale, while the Big Data is the term used to do this processing but on a large scale. Therefore, Big Data is only a small part of what is done in Data Science, although it certainly also requires the help of other professionals such as Data Engineers or Data Architect who support that great computing capacity that data science requires. \n",
    "\n",
    "\n",
    "### So how are data science and Big Data related to machine learning and artificial intelligence?\n",
    "\n",
    "Well, to train these very precise models, large amounts of data are required and this is where the work of the data scientist and data engineer comes in, taking the previous diagram to the next step:\n",
    "<img src=\"images/ds_big_data.png\" alt=\"diagram\" width=\"500\"/>\n",
    "\n",
    "\n",
    "With this introduction, we proceed to solve the next notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c20b3d",
   "metadata": {},
   "source": [
    "## What we will learn in this notebook?\n",
    "\n",
    "- Types of data.\n",
    "- Metricas estadisticas.\n",
    "- Quantile vs Percentile.\n",
    "- Data visualization: One variable.\n",
    "- Data visualization: Two variable.\n",
    "- Data visualization: Three variable.\n",
    "\n",
    "## What is the goal of this notebook?\n",
    "\n",
    "Although in the other lessons we have learned to deal with real data, in this notebook we will define exactly and put into context fundamental concepts in the area of data science so that you can carry out a preliminary analysis of the data to then adjust a machine learning model that we will learn throughout the bootcamp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6edd5c1",
   "metadata": {},
   "source": [
    "## Types of variable\n",
    "\n",
    "#### What is a data?\n",
    "We can define that a data is any vector that we can read with a computer. So, under this definition, everything we mention below is a data:\n",
    "\n",
    "- Images\n",
    "- Words\n",
    "- Audios\n",
    "- Tables\n",
    "- Vectors\n",
    "\n",
    "In any case, We always seek to bring everything to a simple vector or array. But actually, how can an image be an array?\n",
    "\n",
    "<img src=\"images/im_array.png\" alt=\"diagram\" width=\"600\"/>\n",
    "\n",
    "An image is just a composition of three channels: Red, Green, Blue (RGB) with colors from 0 to 255 (256 colors). So, as you can see, you just have to bring each data to an array and that's why `numpy`is so important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebe8a06",
   "metadata": {},
   "source": [
    "#### Read the image \"4geeks_black.jpg\" and get the dimensions of the array (★☆☆)\n",
    "Use `cv2.imread`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a758bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a708990",
   "metadata": {},
   "source": [
    "#### Plot the image using matplotlib (★☆☆)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477e01f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b16cd77",
   "metadata": {},
   "source": [
    "#### Now try the same (read and plot) with the image \"4geeks_blue.png\" (★★☆) \n",
    "Is the image blue? Does it look exactly how it should be? If not, try to analyze why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9b5975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "782d4c77",
   "metadata": {},
   "source": [
    "#### Change the order of the array using numpy (★★☆)\n",
    "Please try `[:,:,::-1]` and response why didn't we have that problem with the 4geeks_black image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe1a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48d48f01",
   "metadata": {},
   "source": [
    "#### Now change the order of the channels using openCV (★★☆)\n",
    "Use `cv2.cvtColor` and `cv2.COLOR_BGR2RGB`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed233e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fa39382",
   "metadata": {},
   "source": [
    "#### Read the dataset titanic_train.csv using pandas and make a description (★☆☆)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97757ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da64551a",
   "metadata": {},
   "source": [
    "## Statistics is the science that studies how information should be used and then respond to practical situations that involve uncertainty.\n",
    "\n",
    "Statistics deals with obtaining research conclusions through the use of mathematical models, providing a methodology through which the discrepancies between what is observed and what is predicted by the model can be evaluated and judged (inference), all this based on information contained in a data set.\n",
    "\n",
    "## Statistical inference vs descriptive statistics\n",
    "\n",
    "<img src=\"images/pop_samp.png\" alt=\"diagram\" width=\"600\"/>\n",
    "\n",
    "Descriptive Statistics provides the techniques to summarize and present the information extracted from a sample. However, we are rarely interested in the sample as such, but rather because of its ability to provide information about other subjects or other situations.\n",
    "Inferential Statistics provides the techniques to draw conclusions from a sample.\n",
    "\n",
    "## Basic metrics for descriptive statistics\n",
    "\n",
    "- Mean\n",
    "- Median\n",
    "- Variance\n",
    "- Standard deviation\n",
    "- Quantiles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f989e6d",
   "metadata": {},
   "source": [
    "## Definitions:\n",
    "\n",
    "- Mean: The sample mean, also called the sample arithmetic mean or simply the average, is the arithmetic average of all the items in a dataset. The mean of a dataset 𝑥 is mathematically expressed as $\\sum_i \\frac{x_i}{n}$, where $𝑖 = 1, 2, \\ldots, n$ . In other words, it’s the sum of all the elements 𝑥ᵢ divided by the number of items in the dataset $x$.\n",
    "\n",
    "- Median: The sample median is the middle element of a sorted dataset. The dataset can be sorted in increasing or decreasing order. If the number of elements 𝑛 of the dataset is odd, then the median is the value at the middle position: $0.5(n + 1)$. If $n$ is even, then the median is the arithmetic mean of the two values in the middle, that is, the items at the positions $0.5n$ and $0.5𝑛 + 1$.\n",
    "\n",
    "- Variance: The sample variance quantifies the spread of the data. It shows numerically how far the data points are from the mean. You can express the sample variance of the dataset $x$ with $n$ elements mathematically as $s^2 = \\sum_i \\frac{(x_i - \\bar{x})^2}{n-1}$, where $i = 1, 2, \\ldots, n$ and $\\bar{x}$ is the sample mean of $x$. We will undestand deeper If you want to understand deeper why you divide the sum with $n − 1$ instead of $n$ in the bootcamp.\n",
    "\n",
    "\n",
    "\n",
    "- Standard deviation: The sample standard deviation is another measure of data spread. It’s connected to the sample variance, as standard deviation, $s$, is the positive square root of the sample variance. The standard deviation is often more convenient than the variance because it has the same unit as the data points. Once you get the variance, you can calculate the standard deviation with pure Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ab5f03",
   "metadata": {},
   "source": [
    "## Percentiles\n",
    "\n",
    "The sample $p$ percentile is the element in the dataset such that $p\\%$ of the elements in the dataset are less than or equal to that value. Also, $(100 − p)\\%$ of the elements are greater than or equal to that value. If there are two such elements in the dataset, then the sample 𝑝 percentile is their arithmetic mean. Each dataset has three quartiles, which are the percentiles that divide the dataset into four parts:\n",
    "\n",
    "The first quartile is the sample 25th percentile. It divides roughly 25% of the smallest items from the rest of the dataset.\n",
    "The second quartile is the sample 50th percentile or the median. Approximately 25% of the items lie between the first and second quartiles and another 25% between the second and third quartiles.\n",
    "The third quartile is the sample 75th percentile. It divides roughly 25% of the largest items from the rest of the dataset.\n",
    "\n",
    "## Quantiles\n",
    "\n",
    "What's the difference with between quantiles and percentiles? \n",
    "\n",
    "The definition is almost the same, only in the case of quantiles, we seek to divide the sample into 5 parts, we start with the $q20$ which means it  divides roughly 20% of the smallest items from the rest of the dataset and we end with $q80$ which means it divides roughly 80% of the largest items from the rest of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f76de",
   "metadata": {},
   "source": [
    "#### Calculate the mean, median, variance and standar deviation of the variable \"Age\" using two different methods (★★☆) \n",
    "Use `np.sum` for example or `statistics.mean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327f7adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89b548e3",
   "metadata": {},
   "source": [
    "#### Calculate all percentiles and all quantiles using two different methods (★★☆) \n",
    "Use `numpy`and `statistics`libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba51bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5056596",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "\n",
    "Unquestionably, the world we live in is a three-dimensional space. So that means that at most, we can do a data visualization like the one we see below:\n",
    "<img src=\"images/3d.png\" alt=\"diagram\"/>\n",
    "\n",
    "But what if we tell you we can make 4d data visualization? and what about 5d data visualization?\n",
    "\n",
    "that's exactly what we will do next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86dc493",
   "metadata": {},
   "source": [
    "#### Run the following lines of code (★☆☆) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5280d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x = np.random.standard_normal(100)\n",
    "y = np.random.standard_normal(100)\n",
    "z = np.random.standard_normal(100)\n",
    "c = np.random.standard_normal(100)\n",
    "\n",
    "img = ax.scatter(x, y, z, c=c, cmap=plt.hot())\n",
    "fig.colorbar(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090a6a3e",
   "metadata": {},
   "source": [
    "#### As you can see, we can change the colors of the scatter plot points and \"add\" an extra dimension to our plot and better understand our data\n",
    "\n",
    "#### Read the dataset \"data_4d.csv\" and make a 4d plot by each class given (★★★) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aec11c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
